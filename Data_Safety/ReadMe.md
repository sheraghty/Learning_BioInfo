# Overview #
This directory contains several vignettes on various ways to promote safe data management. See below for some general recommendations and best practices. Also below is a brief description of each the topics covered by each vignette.
# Best Practices in Data Safety #
## The 3-2-1 rule ##
This rule provides guidance on how to best store data and ensure its safety. In the context of bioinformatics, its best to apply this rule to sequencing data (although it really should be applied to any raw data being used in analyses e.g. morphological data or sample information). The 3 refers to the number of copies of data that should exist. This means that all raw data should exist in triplicate. The 2 refers to the number of different storage media (e.g. cloud storage or on removable hard drive) and the 1 refers to the number of copies that should exist “offsite” (e.g. not in the lab). The meaning of offsite here can be a little nuanced. Your office/lab may also be in approximately the same location as where the servers for your computational platform are, so the data stored on your servers and a hard driver in the office/lab are effectively in the same place. Basically, the spirit of this rule is that if there is some sort of catastrophic event, like a fire or earthquake, then the offsite copy should be far enough way to be safe. A good guideline might be that offsite should mean that it is not located on the same research campus where you conduct your research. 
There are some other considerations as well that may be relevant depending on the nature of the data you have. For instance, some guidelines also add that caveat that one (or more) of the data copies should be “air gapped”, which means on a storage medium that is not connected to the internet (e.g. a removable drive). This copy should also be made to be immutable, or in other words it needs to be made such that it cannot be edited. One final note of caution, when making back-ups it is important to make sure that the process finishes without errors. 
### An example of the 3-2-1 rule in action ### 
To provide an example that might be relevant for bioinformatics scientists, imagine a scenario where we receive a whole genome sequencing (WGS) dataset that is ~ 200 Gb in size. Before you begin to analyze the data, it is important to immediately backup the raw data following the 3-2-1 rule. We might accomplish this by creating one copy to be stored on a removable hard drive that is kept either in our office or lab space and then uploading the data to NCBI as well (remember, the data can be uploaded and embargoed until the project is ready to publish). Finally, we upload the data to onto whatever computer server that we are using for data analysis. 

In this example, you comply with the 3-2-1 rule in the following ways
* There are a total of 3 data copies – the hard drive, NCBI, and your computational resources
* There are at least two different storage media – hard drive and cloud (NCBI) 
*	One copy (the data on NCBI) is located offsite (in this scenario lets assume we don’t work for NCBI)

## A Digital lab notebook ##
Keeping the raw data for a project is of the utmost importance given that it’s the starting point for all analyses. It is equally important to keep a good record of how the data is being manipulated for two reasons. One, if you do experience some sort of data loss, you don’t want to have to proverbially recreate the wheel to get back to where you previously were. Secondly, this is simply apart of good reproducible science. Providing the code used in analyses is common practice when publishing and the more detailed your scripts and workflows are, the more likely a random person can recreate your results. 
A good digital lab notebook is more than a collection of loose scripts in the various directories that you are using for your data analyses, you also need to include instructions for the order in which scripts and various commands are run and why. Additionally, we need to follow some of the same principles for as in the 3-2-1 rule for keeping our notes. Namely, we want to multiple copies located in different places. Again, this is useful in restoring progress after a data loss event because we won’t have to remake a bunch of scripts. Ideally, a good bioinformatics scientist will leave plenty of README files in the directories that they are working in that provide a description of the workflow. Additionally, they would use something like GitHub to save their scripts and write detailed instructions for their usage. 

# Tutorials #
- [Permissons](Permissons.md): This tutorial goes over the basics of how to set permissions for different files and directories using the `chmod` command.
- [Using links](Using_links.md): This tutorial goes over how to set links with the `ln` command.
- [Record workflow](Recording_workflow.md): This tutorial goes over how to record shell activities using both the `history` and `script` commands. 
